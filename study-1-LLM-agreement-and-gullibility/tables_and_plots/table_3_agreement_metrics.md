
Markdown:
| Model          | Prompt    |   Cohen's Kappa (on Binary Labels) |   Krippendorff's Alpha (on Graded Labels) |   MAE (Binary) |   MAE (Graded) |   Accuracy |   Prec(Label=0) |   Prec(Label=1) |   P(Label=1) |
|:---------------|:----------|-----------------------------------:|------------------------------------------:|---------------:|---------------:|-----------:|----------------:|----------------:|-------------:|
| Claude-3 Haiku | Basic     |                               0.06 |                                      0.07 |           0.47 |           1.03 |       0.53 |            0.7  |            0.36 |         0.51 |
| Claude-3 Haiku | Rationale |                               0.23 |                                      0.15 |           0.45 |           1.18 |       0.55 |            0.96 |            0.42 |         0.77 |
| Claude-3 Haiku | Utility   |                               0.16 |                                      0    |           0.52 |           1.28 |       0.48 |            0.97 |            0.39 |         0.84 |
| Claude-3 Opus  | Basic     |                               0.37 |                                      0.41 |           0.34 |           0.82 |       0.66 |            0.92 |            0.49 |         0.61 |
| Claude-3 Opus  | Rationale |                               0.49 |                                      0.48 |           0.25 |           0.77 |       0.75 |            0.91 |            0.58 |         0.5  |
| Claude-3 Opus  | Utility   |                               0.28 |                                      0.24 |           0.41 |           1.05 |       0.59 |            0.94 |            0.44 |         0.71 |
| Command-R      | Basic     |                               0.09 |                                      0    |           0.58 |           1.16 |       0.42 |            0.97 |            0.36 |         0.91 |
| Command-R      | Rationale |                               0.14 |                                     -0    |           0.53 |           1.26 |       0.47 |            0.96 |            0.38 |         0.85 |
| Command-R      | Utility   |                               0.15 |                                     -0.02 |           0.52 |           1.3  |       0.48 |            0.96 |            0.39 |         0.84 |
| Command-R+     | Basic     |                               0.16 |                                      0.1  |           0.51 |           1.24 |       0.49 |            0.98 |            0.39 |         0.84 |
| Command-R+     | Rationale |                               0.29 |                                      0.25 |           0.4  |           1.08 |       0.6  |            0.93 |            0.45 |         0.7  |
| Command-R+     | Utility   |                               0.22 |                                      0.24 |           0.47 |           1    |       0.53 |            0.97 |            0.41 |         0.78 |
| LLaMA3 8B      | Basic     |                               0.27 |                                      0.22 |           0.41 |           0.86 |       0.59 |            0.92 |            0.44 |         0.7  |
| LLaMA3 8B      | Rationale |                               0.35 |                                      0.32 |           0.33 |           0.94 |       0.67 |            0.87 |            0.5  |         0.55 |
| LLaMA3 8B      | Utility   |                               0.17 |                                      0.06 |           0.51 |           0.96 |       0.49 |            0.95 |            0.39 |         0.82 |
| LLaMA3 70B     | Basic     |                               0.37 |                                      0.45 |           0.34 |           0.81 |       0.66 |            0.94 |            0.49 |         0.63 |
| LLaMA3 70B     | Rationale |                               0.41 |                                      0.45 |           0.31 |           0.81 |       0.69 |            0.94 |            0.52 |         0.59 |
| LLaMA3 70B     | Utility   |                               0.33 |                                      0.36 |           0.37 |           0.95 |       0.63 |            0.94 |            0.47 |         0.67 |
| GPT-3.5-turbo  | Basic     |                               0.26 |                                      0.21 |           0.42 |           1.07 |       0.58 |            0.93 |            0.44 |         0.71 |
| GPT-3.5-turbo  | Rationale |                               0.36 |                                      0.33 |           0.34 |           1    |       0.66 |            0.91 |            0.49 |         0.59 |
| GPT-3.5-turbo  | Utility   |                               0.23 |                                      0.18 |           0.45 |           0.91 |       0.55 |            0.93 |            0.42 |         0.74 |
| GPT-4          | Basic     |                               0.47 |                                      0.5  |           0.27 |           0.78 |       0.73 |            0.92 |            0.56 |         0.53 |
| GPT-4          | Rationale |                               0.49 |                                      0.57 |           0.22 |           0.64 |       0.78 |            0.82 |            0.68 |         0.31 |
| GPT-4          | Utility   |                               0.42 |                                      0.44 |           0.3  |           0.86 |       0.7  |            0.93 |            0.53 |         0.57 |
| GPT-4o         | Basic     |                               0.52 |                                      0.63 |           0.21 |           0.61 |       0.79 |            0.84 |            0.69 |         0.32 |
| GPT-4o         | Rationale |                               0.54 |                                      0.62 |           0.21 |           0.64 |       0.79 |            0.87 |            0.65 |         0.38 |
| GPT-4o         | Utility   |                               0.52 |                                      0.62 |           0.22 |           0.61 |       0.78 |            0.88 |            0.63 |         0.41 |
LaTeX:
\begin{tabular}{llrrrrrrrr}
\toprule
         Model &    Prompt &  Cohen's Kappa (on Binary Labels) &  Krippendorff's Alpha (on Graded Labels) &  MAE (Binary) &  MAE (Graded) &  Accuracy &  Prec(Label=0) &  Prec(Label=1) &  P(Label=1) \\
\midrule
Claude-3 Haiku &     Basic &                              0.06 &                                     0.07 &          0.47 &          1.03 &      0.53 &           0.70 &           0.36 &        0.51 \\
Claude-3 Haiku & Rationale &                              0.23 &                                     0.15 &          0.45 &          1.18 &      0.55 &           0.96 &           0.42 &        0.77 \\
Claude-3 Haiku &   Utility &                              0.16 &                                     0.00 &          0.52 &          1.28 &      0.48 &           0.97 &           0.39 &        0.84 \\
 Claude-3 Opus &     Basic &                              0.37 &                                     0.41 &          0.34 &          0.82 &      0.66 &           0.92 &           0.49 &        0.61 \\
 Claude-3 Opus & Rationale &                              0.49 &                                     0.48 &          0.25 &          0.77 &      0.75 &           0.91 &           0.58 &        0.50 \\
 Claude-3 Opus &   Utility &                              0.28 &                                     0.24 &          0.41 &          1.05 &      0.59 &           0.94 &           0.44 &        0.71 \\
     Command-R &     Basic &                              0.09 &                                     0.00 &          0.58 &          1.16 &      0.42 &           0.97 &           0.36 &        0.91 \\
     Command-R & Rationale &                              0.14 &                                    -0.00 &          0.53 &          1.26 &      0.47 &           0.96 &           0.38 &        0.85 \\
     Command-R &   Utility &                              0.15 &                                    -0.02 &          0.52 &          1.30 &      0.48 &           0.96 &           0.39 &        0.84 \\
    Command-R+ &     Basic &                              0.16 &                                     0.10 &          0.51 &          1.24 &      0.49 &           0.98 &           0.39 &        0.84 \\
    Command-R+ & Rationale &                              0.29 &                                     0.25 &          0.40 &          1.08 &      0.60 &           0.93 &           0.45 &        0.70 \\
    Command-R+ &   Utility &                              0.22 &                                     0.24 &          0.47 &          1.00 &      0.53 &           0.97 &           0.41 &        0.78 \\
     LLaMA3 8B &     Basic &                              0.27 &                                     0.22 &          0.41 &          0.86 &      0.59 &           0.92 &           0.44 &        0.70 \\
     LLaMA3 8B & Rationale &                              0.35 &                                     0.32 &          0.33 &          0.94 &      0.67 &           0.87 &           0.50 &        0.55 \\
     LLaMA3 8B &   Utility &                              0.17 &                                     0.06 &          0.51 &          0.96 &      0.49 &           0.95 &           0.39 &        0.82 \\
    LLaMA3 70B &     Basic &                              0.37 &                                     0.45 &          0.34 &          0.81 &      0.66 &           0.94 &           0.49 &        0.63 \\
    LLaMA3 70B & Rationale &                              0.41 &                                     0.45 &          0.31 &          0.81 &      0.69 &           0.94 &           0.52 &        0.59 \\
    LLaMA3 70B &   Utility &                              0.33 &                                     0.36 &          0.37 &          0.95 &      0.63 &           0.94 &           0.47 &        0.67 \\
 GPT-3.5-turbo &     Basic &                              0.26 &                                     0.21 &          0.42 &          1.07 &      0.58 &           0.93 &           0.44 &        0.71 \\
 GPT-3.5-turbo & Rationale &                              0.36 &                                     0.33 &          0.34 &          1.00 &      0.66 &           0.91 &           0.49 &        0.59 \\
 GPT-3.5-turbo &   Utility &                              0.23 &                                     0.18 &          0.45 &          0.91 &      0.55 &           0.93 &           0.42 &        0.74 \\
         GPT-4 &     Basic &                              0.47 &                                     0.50 &          0.27 &          0.78 &      0.73 &           0.92 &           0.56 &        0.53 \\
         GPT-4 & Rationale &                              0.49 &                                     0.57 &          0.22 &          0.64 &      0.78 &           0.82 &           0.68 &        0.31 \\
         GPT-4 &   Utility &                              0.42 &                                     0.44 &          0.30 &          0.86 &      0.70 &           0.93 &           0.53 &        0.57 \\
        GPT-4o &     Basic &                              0.52 &                                     0.63 &          0.21 &          0.61 &      0.79 &           0.84 &           0.69 &        0.32 \\
        GPT-4o & Rationale &                              0.54 &                                     0.62 &          0.21 &          0.64 &      0.79 &           0.87 &           0.65 &        0.38 \\
        GPT-4o &   Utility &                              0.52 &                                     0.62 &          0.22 &          0.61 &      0.78 &           0.88 &           0.63 &        0.41 \\
\bottomrule
\end{tabular}
