# On the Use of LLMs for Relevance Labelling

This repository contains the data required to reproduce the results from our paper:  
**‚ÄúOn the Use of LLMs for Relevance Labelling‚Äù**, submitted for review at TOIS.

---

## üìå Introduction

This repository supports reproducibility of the experiments reported in our study. It includes LLM-generated labels, manipulated passage files for gullibility tests, and the LLM-generated qrels for TREC runs. The work investigates the reliability, failure modes, and biases of LLMs when used for relevance judgement in IR evaluation.

---

## üìä Study 1 ‚Äì LLM and Human Labelling Agreement

The folder `study-1-LLM-agreement-and-gullibility` contains labels generated by multiple LLMs across various relevance labelling and gullibility test setups. All files are in `.csv` format and follow this naming format:


```
[LLMNameAndVersion]_prompt_[PromptName]_[Collection].csv
```

The table below maps the `PromptName` used in the file naming convention to its corresponding name used in the paper:

| **PromptName** | **Mapping** |
|----------------|-------------|
| `simple`       | Basic       |
| `Upadhyay`     | Rationale   |
| `thomas`       | Utility     |

Most fields are self-explanatory, but here are two important ones that are used in all files (others are explained in the next table):

- **`response` field**: This contains the raw response generated by the LLM.  
- **`O_score` field**: This contains the label parsed from the raw response.

Files are flat and contain the original (and manipulated passages in the case of gullibility tests) so analysis can be done directly to reproduce the results.


### üìÅ File Organization

The files and scripts in this repository are organised into the following folders:

| **Folder Name**                                | **Description**                                                                                                                                                                                                                                                                                                               |
|------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `LLM_relevance_labelling`                      | Contains LLM responses for labelling the relevance of actual DL21 and DL22 query-passage pairs (used to reproduce Fig-2 and Table-3).                                                                                                                                                                                         |
| `non-relevant_with_query_injection_files`      | Contains LLM responses for keyword stuffing gullibility tests on non-relevant passages (scored 0 by NIST human assessors). The `field` values indicate the type of stuffing: - `passage_and_query`: Full query string injected at a random position. - `passage_and_query_words`: Query words placed randomly in the passage. |
| `non-relevant_with_score_description_injection_files` | Contains LLM responses for instruction stuffing gullibility tests on non-relevant passages (scored 0 by NIST human assessors).                                                                                                                                                                                          |
| `random_passage_with_query_injection_files`    | Contains LLM responses for keyword stuffing gullibility tests on random passages. The `field` values indicate passage length and type of stuffing: - `brown_random_text_with_query_100`: A random text of 100 words with the query string injected into it.                                                                   |
| `random_passage_with_score_injection_files`    | Same as above but for instruction stuffing tests.                                                                                                                                                                                                                                                                       |


## üìä Study 2 ‚Äì System Ranking Agreement

The folder `study-2-TREC-run-LLM-labels` contains qrels generated using nine different LLMs for the DL21 and DL22 test collections. These were used to evaluate all participating systems and compare LLM- vs. human-based ranking outcomes.

**File naming convention**:
`[Collection]_[LLMNameAndVersion]_qrels.txt`

These files follow the standard TREC qrels format and can be used with the official TREC evaluation script.
