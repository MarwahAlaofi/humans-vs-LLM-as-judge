## Introduction

This repository contains the data to reproduce the results associated with our submitted paper for review. 

### Study 1 - LLM AND HUMAN LABELLING AGREEMENT 
The folder `study-1-LLM-agreement-and-gullibility` contains the labels generated by LLMs. Files are saved in `.csv` files. The file naming follows this structure:

```
[LLMNameAndVersion]_prompt_[PromptName]_[Collection].csv
```

The table below maps the `PromptName` used in the file naming convention to its corresponding name used in the paper:

| **PromptName** | **Mapping** |
|----------------|-------------|
| `simple`       | Basic       |
| `Upadhyay`     | Rationale   |
| `thomas`       | Utility     |

Most fields are self-explanatory, but here are two important ones that are used in all files (others are explained in the next table):

- **`response` field**: This contains the raw response generated by the LLM.  
- **`O_score` field**: This contains the label parsed from the raw response.

Files are flat and contain the original (and manipulated passages in the case of gullibility tests) so analysis can be done directly to reproduce the results.


#### File Organization

The files and scripts in this repository are organised into the following folders:

| **Folder Name**                                | **Description**                                                                                                                                                                                                                                                                                                               |
|------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `LLM_relevance_labelling`                      | Contains LLM responses for labelling the relevance of actual DL21 and DL22 query-passage pairs (used to reproduce Fig-2 and Table-3).                                                                                                                                                                                         |
| `non-relevant_with_query_injection_files`      | Contains LLM responses for keyword stuffing gullibility tests on non-relevant passages (scored 0 by NIST human assessors). The `field` values indicate the type of stuffing: - `passage_and_query`: Full query string injected at a random position. - `passage_and_query_words`: Query words placed randomly in the passage. |
| `non-relevant_with_score_description_injection_files` | Contains LLM responses for instruction stuffing gullibility tests on non-relevant passages (scored 0 by NIST human assessors).                                                                                                                                                                                          |
| `random_passage_with_query_injection_files`    | Contains LLM responses for keyword stuffing gullibility tests on random passages. The `field` values indicate passage length and type of stuffing: - `brown_random_text_with_query_100`: A random text of 100 words with the query string injected into it.                                                                   |
| `random_passage_with_score_injection_files`    | Same as above but for instruction stuffing tests.                                                                                                                                                                                                                                                                       |


### Study 2 - SYSTEM RANKING AGREEMENT
The folder `study-2-TREC-run-LLM-labels` contains the qrels generated using the nine LLMs across the two collections DL21 and DL22. Files follow the naming format:
`[Collection]_[LLM]_qrels.txt`
